{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction Methods in Spark for Text Data and Numeric Data\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relvant libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basic Feature Extraction').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating text data\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\"),\n",
    "], [\"label\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Feature Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting words from sentence\n",
    "tokenizer = Tokenizer(inputCol = \"sentence\", outputCol = \"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "#Coverting words into raw features\n",
    "hashingTF = HashingTF(inputCol = \"words\", outputCol = \"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "#Converting raw features to numerics\n",
    "idf = IDF(inputCol = \"rawFeatures\", outputCol = 'features')\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec : Words to vector method\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"HI I heard about Saprk\".split(\" \"),),\n",
    "    (\"I wish java could use case classes\".split(\" \"),),\n",
    "    (\"Logistic regressions models are neat\".split(\" \"),)\n",
    "], [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "word2vect = Word2Vec(vectorSize = 3, minCount = 0, inputCol = \"text\", outputCol = \"result\")\n",
    "model = word2vect.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\n Vector: %s\\n\" % (\",  \".join(text), str(vector)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onehot encoding\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import oneHotEncoder\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0.0,1.0),\n",
    "    (1.0,0.0),\n",
    "    (2.0,1.0),\n",
    "    (0.0,2.0),\n",
    "    (0.0,1.0),\n",
    "    (2.0,0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = oneHotEncoder(inputCol = [\"categoryIndex1\", \"categoryIndex2\"], outputCol = [\"categoryvec1\", \"categoryvec2\"])\n",
    "\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transfom(df)\n",
    "model.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Basic Feature Extraction').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.read.format('libsvm').load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "scaler = StandardScaler(inputCol = 'features', outputCol = \"scaledFeatures\", withStd = True, withMean = False)\n",
    "\n",
    "scalerModel = scaler.fit(dataframe)\n",
    "\n",
    "scaledData = scalerModel.transform(dataframe)\n",
    "scaledData.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dea71f8f2a6711d6d84e235248e13da33741ca071b6ac1f9277e3f0ea2400a15"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
